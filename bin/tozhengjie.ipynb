{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####导入模块\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##将已有的词向量字典文件load\n",
    "dir = \"????/kaggle/data\"\n",
    "wordvecfile = \"wordsvector.txt\"\n",
    "path = os.path.join(dir, wordvecfile)\n",
    "fd = open(path)\n",
    "line = fd.readline()\n",
    "wordvecdic = json.loads(line)\n",
    "fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###打印时间函数  用于函数调用\n",
    "def log_time(s):\n",
    "    now = datetime.datetime.now()\n",
    "    print \"[%s][%s.%s][%s]\" %(time.strftime('%Y-%m-%d %H-%M-%S', time.localtime(time.time())), now.second, now.microsecond, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####句（词list）转为句向量　　用于函数调用\n",
    "def sentence_vecor(wordlist):\n",
    "    vector = []\n",
    "    for word in wordlist:\n",
    "        try:\n",
    "            vector.append(wordvecdic[word])\n",
    "        except Exception,e:\n",
    "            #log_time(e)\n",
    "            vector.append(np.random.uniform(-0.25,0.25,50).tolist())\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###文件转为数据，返回为多个多维数组　node为训练（＆验证）与　test数据标识\n",
    "###node == train：返回dataset1, dataset2, labels　\n",
    "###node == test: 返回dataset1, dataset2\n",
    "def generate_dataset(datafile, samplesize, node):\n",
    "    fd = open(datafile)\n",
    "    dataset1 = np.zeros((samplesize, 300, 50), dtype=np.float32)\n",
    "    dataset2 = np.zeros((samplesize, 300, 50), dtype=np.float32)\n",
    "    if node == \"train\":\n",
    "        labels = np.zeros(samplesize, dtype = np.int32)\n",
    "        i = 0\n",
    "        for line in fd:\n",
    "            data = line.split(\"\\t\")\n",
    "            question1, question2, is_duplicate = json.loads(data[0]), json.loads(data[1]), data[2]\n",
    "            dataset1[i][:len(question1)] = sentence_vecor(question1)\n",
    "            dataset2[i][:len(question2)] = sentence_vecor(question2)\n",
    "            labels[i] = int(is_duplicate)\n",
    "            i += 1\n",
    "        fd.close()\n",
    "        return dataset1, dataset2, labels\n",
    "    elif node == \"test\":\n",
    "        dataset1 = np.zeros((samplesize, 300, 50), dtype=np.float32)\n",
    "        dataset2 = np.zeros((samplesize, 300, 50), dtype=np.float32)\n",
    "        i = 0\n",
    "        for line in fd:\n",
    "            data = line.split(\"\\t\")\n",
    "            question1, question2 = json.loads(data[0]), json.loads(data[1])\n",
    "            dataset1[i][:len(question1)] = sentence_vecor(question1)\n",
    "            dataset2[i][:len(question2)] = sentence_vecor(question2)\n",
    "            i += 1\n",
    "        fd.close()\n",
    "        return dataset1, dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##输入是filepath，code(\"train\":数据集为dataset1,dataset2,labels \"test\":数据集为dataset1,dataset2)\n",
    "##例如：　（/home/lixuelian/lillian/kaggle/data/test, \"test\") 将该路径下的所有文件置换为数据集，用pickle.load反序列化字典对象\n",
    "def filepath_dataset(filepath, code):\n",
    "    log_time(\"begin\")\n",
    "    files = os.listdir(filepath)\n",
    "    if code == \"train\":\n",
    "        for trainfile in files:\n",
    "            trainfile = os.path.join(filepath, trainfile)\n",
    "            f = open(trainfile)\n",
    "            column = len(f.readlines())\n",
    "            f.close()\n",
    "            try:\n",
    "                train_dataset1, train_dataset2, train_labels = generate_dataset(trainfile, column, \"train\")\n",
    "                ##替换掉\n",
    "                if os.path.exists(trainfile): os.remove(trainfile)\n",
    "                f = open(trainfile, 'wb')\n",
    "                save = {'train_dataset1': train_dataset1, 'train_dataset2': train_dataset2, 'train_labels': train_labels}\n",
    "                pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "                f.close()\n",
    "                del train_dataset1, train_dataset2, train_labels\n",
    "            except Exception as e:\n",
    "                log_time(trainfile + \": is not succesed\")\n",
    "    elif code == \"test\":\n",
    "        for testfile in files:\n",
    "            testfile = os.path.join(filepath, testfile)\n",
    "            f = open(testfile)\n",
    "            column = len(f.readlines())\n",
    "            f.close()\n",
    "            try:\n",
    "                test_dataset1, test_dataset2 = generate_dataset(testfile, column, \"test\")\n",
    "                ###替换掉\n",
    "                if os.path.exists(testfile): os.remove(testfile)\n",
    "                f = open(testfile, 'wb')\n",
    "                save = {'test_dataset1': test_dataset1, 'test_dataset2': test_dataset2}\n",
    "                pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "                f.close()\n",
    "                del test_dataset1, test_dataset2\n",
    "            except Exception as e:\n",
    "                log_time(testfile + \": is not succesed\")\n",
    "    log_time(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###数组字典文件　load\n",
    "def load_dataset(filename, node):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        try:\n",
    "            if node == \"train\":\n",
    "                ##第一个句子的样本矩阵　为np.ndarray((samplesize, 300, 50), dtype=np.float32))\n",
    "                train_dataset1 = data['train_dataset1'] \n",
    "                ##第二个句子的样本矩阵　为np.ndarray((samplesize, 300, 50), dtype=np.float32))\n",
    "                train_dataset2 = data['train_dataset2']\n",
    "                ##类别标识　np.ndarray((samplesize, dtype = np.int32))\n",
    "                train_labels = data['train_labels']\n",
    "                return train_dataset1, train_dataset2, train_labels\n",
    "            elif node == \"test\":\n",
    "                test_dataset1 = data[\"test_dataset1\"]\n",
    "                test_dataset2 = data[\"test_dataset2\"]\n",
    "                return test_dataset1, test_dataset2\n",
    "        except:\n",
    "            log_time(filename + \":load failed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
