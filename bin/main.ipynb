{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gensim\n",
    "import sys\n",
    "import datetime,time\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_time_model(s):\n",
    "    now = datetime.datetime.now()\n",
    "    print \"[%s][%s.%s][%s]\" %(time.strftime('%Y-%m-%d %H-%M-%S', time.localtime(time.time())), now.second, now.    microsecond, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape(\n",
    "        (-1, WordVec_Length, Word_Length_Default * 2,num_channels)).astype(np.float32)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reAdd(dataset,Filter_Width):\n",
    "    # give pairs's distance,then distance = Filter_Length * File_Width \n",
    "    # and  return real input data x  =   [None, WordVec_Length, Word_Length_Default * 2 + Filter_Length,num_channels]\n",
    "    b = np.zeros([Filter_Length,Filter_Width,num_channels])\n",
    "    return np.insert(dataset, Word_Length_Default, values=b, axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-04-21 19-15-57][57.986780][x.shape()TensorShape([Dimension(2), Dimension(50), Dimension(103), Dimension(1)])]\n"
     ]
    }
   ],
   "source": [
    "# define wordvec parameter \n",
    "WordVec_Length = 50\n",
    "WordVec_Depth = 1\n",
    "\n",
    "# define batch size \n",
    "BatchSize = 2\n",
    "\n",
    "# define filter parameter\n",
    "Filter_Length = WordVec_Length\n",
    "Filter_Width = 3\n",
    "num_channels = WordVec_Depth\n",
    "### define filter depth, then equal next layer filter_curmat_depth\n",
    "Filter_Depth = 9\n",
    "\n",
    "# one question 's max word length , is equal with lenth of sentence.\n",
    "Word_Length_Default = 50\n",
    "\n",
    "x = tf.placeholder(tf.float32,[BatchSize, WordVec_Length, Word_Length_Default * 2 + Filter_Width,num_channels],name='x-input1')\n",
    "#  dimension[0] is batch size,dim[1] and dim[2] means pairs's size,dim[3] is depth of pairs\n",
    "\n",
    "log_time_model(\"x.shape()\" + str(x.get_shape()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-04-21 19-15-58][58.58618][conv1_filter_weights = TensorShape([Dimension(51), Dimension(3), Dimension(1), Dimension(9)])]\n",
      "[2017-04-21 19-15-58][58.60744][biases1 = TensorShape([Dimension(9)])]\n",
      "[2017-04-21 19-15-58][58.65574][simM = TensorShape([Dimension(9), Dimension(9)])]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('filter must not be larger than the input: ', 'Filter: [', Dimension(51), 'x', Dimension(3), '] ', 'Input: [', Dimension(50), 'x', Dimension(103), '] ')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-148fb036df66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mlog_time_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"simM = \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv1_filter_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SAME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mlog_time_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv1 = \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/ops/gen_nn_ops.pyc\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, name)\u001b[0m\n\u001b[1;32m    205\u001b[0m   return _op_def_lib.apply_op(\"Conv2D\", input=input, filter=filter,\n\u001b[1;32m    206\u001b[0m                               \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m                               use_cudnn_on_gpu=use_cudnn_on_gpu, name=name)\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/ops/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, g, name, **keywords)\u001b[0m\n\u001b[1;32m    631\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    632\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    634\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         return _Restructure(ops.convert_n_to_tensor_or_indexed_slices(outputs),\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   1711\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1713\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1415\u001b[0m       raise RuntimeError(\"No shape function registered for standard op: %s\"\n\u001b[1;32m   1416\u001b[0m                          % op.type)\n\u001b[0;32m-> 1417\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1418\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/ops/common_shapes.pyc\u001b[0m in \u001b[0;36mconv2d_shape\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    187\u001b[0m   \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m   out_rows, out_cols = _Get2DOutputSize(\n\u001b[0;32m--> 189\u001b[0;31m       in_rows, in_cols, filter_rows, filter_cols, stride, stride, padding)\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/ops/common_shapes.pyc\u001b[0m in \u001b[0;36m_Get2DOutputSize\u001b[0;34m(input_height, input_width, filter_height, filter_width, row_stride, col_stride, padding_type)\u001b[0m\n\u001b[1;32m    105\u001b[0m       raise ValueError(\"filter must not be larger than the input: \",\n\u001b[1;32m    106\u001b[0m                        \u001b[0;34m\"Filter: [\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"x\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"] \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                        \"Input: [\", input_height, \"x\", input_width, \"] \")\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrow_stride\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mfilter_height\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcol_stride\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mfilter_width\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m       raise ValueError(\"stride must be less than or equal to filter size\",\n",
      "\u001b[0;31mValueError\u001b[0m: ('filter must not be larger than the input: ', 'Filter: [', Dimension(51), 'x', Dimension(3), '] ', 'Input: [', Dimension(50), 'x', Dimension(103), '] ')"
     ]
    }
   ],
   "source": [
    "#def inference(input_tensor):\n",
    "with tf.variable_scope('layer1-conv1'):\n",
    "    \n",
    "    conv1_filter_weights = tf.get_variable('weight',\n",
    "                                          [Filter_Length,Filter_Width,num_channels,Filter_Depth],\n",
    "                                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    \n",
    "    log_time_model(\"conv1_filter_weights = \"+str(conv1_filter_weights.get_shape()))\n",
    "\n",
    "    biases1 = tf.get_variable('biases',[Filter_Depth],initializer=tf.constant_initializer(0.1))\n",
    "    \n",
    "    log_time_model(\"biases1 = \"+str(biases1.get_shape()))\n",
    "    \n",
    "    simM = tf.get_variable('M',[Filter_Depth,Filter_Depth],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    # 定义相似度矩阵 simM\n",
    "    \n",
    "    log_time_model(\"simM = \"+str(simM.get_shape()))\n",
    "    \n",
    "    conv1 = tf.nn.conv2d(x, conv1_filter_weights, strides = [1,1,1,1],padding='SAME')\n",
    "    \n",
    "    log_time_model(\"conv1 = \"+str(conv1.get_shape()))\n",
    "\n",
    "    bias1 = tf.nn.bias_add(conv1, biases1)\n",
    "    \n",
    "    log_time_model(\"bias1 = \"+str(bias1.get_shape()))\n",
    "\n",
    "    actived_conv1 = tf.nn.relu(bias1)\n",
    "    \n",
    "    log_time_model(\"actived_conv1 = \"+str(actived_conv1.get_shape()))\n",
    "    \n",
    "    \n",
    "    Conv_Length = 2\n",
    "#     if actived_conv1.get_shape()[2].value == Filter_Depth:\n",
    "#         Conv_Length = actived_conv1.get_shape()[3].value\n",
    "        \n",
    "    pool = tf.nn.max_pool(actived_conv1,ksize=[1,1,Conv_Length/2,1],strides=[1,Conv_Length/2,Conv_Length/2,1],padding='VALID')\n",
    "    \n",
    "    log_time_model(\"pool.get_shape() = \"+ str(pool.get_shape()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # NEXT STEP: MAXPOOL 定义 两个不需要训练的变量，maxpool ()\n",
    "    # 定义一个需要训练的矩阵M\n",
    "    # 两个变量矩阵相乘\n",
    "    # 拉直拼接\n",
    "    # 全连接\n",
    "    # softmax\n",
    "    \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     print sess.run(a.initializer)\n",
    "#     print dir(a.get_shape()[1].value)\n",
    "#     print a.get_shape()[1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read dataset\n",
    "def openFile():\n",
    "    pickle_file = 'dataset.pickle'\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        ##第一个句子的样本矩阵为np.ndarray((samplesize, 300, 50), dtype=np.float32))\n",
    "        train_dataset1 = data['train_dataset1'] \n",
    "        ##第二个句子的样本矩阵为np.ndarray((samplesize, 300, 50), dtype=np.float32))\n",
    "        train_dataset2 = data['train_dataset2']\n",
    "        ##类别标识np.ndarray((samplesize, dtype = np.int32))\n",
    "        train_labels = data['train_labels']\n",
    "        ##验证集\n",
    "        valid_dataset1 = data['valid_dataset1'] \n",
    "        valid_dataset2 = data['valid_dataset2']\n",
    "        valid_labels = data['valid_labels']\n",
    "        ##测试集\n",
    "        test_dataset1 = data['test_dataset1'] \n",
    "        test_dataset2 = data['test_dataset2']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
